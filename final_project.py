# -*- coding: utf-8 -*-
"""Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10YKaGTLDdPD_zUY0b7PKH_Ro3gVvsPS4

#Data cleaning
"""

import pandas as pd
import numpy as np

"""##Reading in data"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/survey.csv')

df

for col in df.columns:
    print(col)

for col in df.columns:
    print(np.sum(df[col].isna()))

"""##Cleaning the 'age' column
For this column, I decided to clip the age range from 18-70, so that there would not be any extreme outliers.
"""

df['Age'] = [age if age >18 else 18 for age in df['Age']]
df['Age'] = [age if age <70 else 70 for age in df['Age']]

df['Age'].unique()

np.sum(df['Age'].isna())

"""##Cleaning the 'gender' column
For this column, I looked at all the unique values and decided to put all these values into 3 categories: male, female, and other.
"""

df['Gender'].unique()

malenames = ["male", "man", "m", "cis male","mal", "male ","cis man","maile","male (cis)","mail","msle","malr"]
femalenames = ["female", "f", "cis female","femake", "woman","cis-female/femme","female (cis)","female "]
for i in range(len(df)):
  if (df.iloc[i, 2].lower() in malenames):
    df.iloc[i, 2] = 'M'
  elif (df.iloc[i, 2].lower() in femalenames):
    df.iloc[i, 2] = 'F'
  else:
    df.iloc[i, 2] = 'O'
#othernames = ["male-ish","trans-female","something kinda male?","queer/she/they","non-binary", "Make", "Nah", "All", "Enby", "fluid", 'Genderqueer']
df['Gender'].unique()

np.sum(df['Gender'].isna())

"""##'Comments' column
Since most values in the Comments column were null, I decided to drop the comments column.
"""

df = df.drop(['comments'], axis = 1)

"""##'Treatment' column
I made sure that all individuals had treatement reported.

"""

np.sum(df['treatment'].isna())

"""##Dropping columns
I dropped columns with too many null values or that were not relevant to the project, such as state, work interfere, and timestamp.
"""

df = df.drop(['state'], axis = 1)

df = df.drop(['work_interfere'], axis = 1)

for col in df.columns:
    print(col)

for col in df.columns:
    print(np.sum(df[col].isna()))

df= df.dropna()

df = df.drop(['Timestamp'], axis = 1)

df.head()

"""##Saving cleaned data"""

df.to_csv('/content/drive/My Drive/survey_cleaned.csv',index =False)

"""#Data Exploration"""

df = pd.read_csv('/content/drive/My Drive/survey_cleaned.csv')

import matplotlib.pyplot as plt
import numpy as np

"""##Finding baseline accuracy"""

df['treatment'].value_counts()

628/(628+613)

"""##Plotting a histogram with treatment values"""

df['treatment'].unique()

list(df['treatment'].value_counts())

df['treatment'].value_counts().index

plt.bar(df['treatment'].value_counts().index, list(df['treatment'].value_counts()))

plt.xlabel("Treatment received")
plt.ylabel("No. of employees receiving treatment")
plt.title("Employees receiving mental health treatment")
plt.show()

"""##Plotting the distribution of ages in the dataset"""

plt.boxplot(df['Age'])
plt.title("Ages of employees")

plt.hist(df['Age'])
plt.title("Ages of employees")

"""##Plotting the number of people in each gender category"""

plt.bar(df['Gender'].unique(), list(df['Gender'].value_counts()))

plt.xlabel("Gender")
plt.ylabel("No. of employees of certain gender")
plt.title("Gender of employees")
plt.show()

x = df['treatment'].unique()
y1 = np.array(df[df['Gender'] == 'M']['treatment'].value_counts(sort=False))
y2 = np.array(df[df['Gender'] == 'F']['treatment'].value_counts(sort=False))
y3 = np.array(df[df['Gender'] == 'O']['treatment'].value_counts(sort=False))

# plot bars in stack manner
plt.bar(x, y1, color='r')
plt.bar(x, y2, bottom=y1, color='b')
plt.bar(x, y3, bottom=y1+y2, color='y')

plt.xlabel("Treatment received")
plt.ylabel("Number of employees")
plt.legend(["Male", "Female", "Other"])
plt.title("Treatment received by gender")
plt.show()

df[df['Gender'] == 'M']['treatment'].value_counts(sort = False)

442/(531+442)

df[df['Gender'] == 'F']['treatment'].value_counts(sort = False)

165/(165+76)

df[df['Gender'] == 'O']['treatment'].value_counts(sort = False)

21/(21+6)

x = df['Gender'].unique()
y1 = np.array(df[df['treatment'] == 'No']['Gender'].value_counts(sort=False))
y2 = np.array(df[df['treatment'] == 'Yes']['Gender'].value_counts(sort=False))

plt.bar(x, y1, color='r')
plt.bar(x, y2, bottom=y1, color='b')

plt.xlabel("Gender")
plt.ylabel("Number of employees")
plt.legend(["No", "Yes"])
plt.title("Treatment received by gender")
plt.show()

"""##Plotting the number of people working in each of the 'number of employees' categories"""

df['no_employees']

x = df['no_employees'].unique()
y = df['no_employees'].value_counts()
plt.figure(figsize=(8,6))
plt.bar(x,y)
plt.title("Number of employees")

"""##Plotting the relationship between age and treatment"""

df['Age'].unique().shape

np.array(df[df['treatment'] == 'Yes']['Age'].value_counts(sort=False)).shape

#x = df['Age'].unique()
y1 = np.array(df[df['treatment'] == 'No']['Age']) #.value_counts(sort=False))
y2 = np.array(df[df['treatment'] == 'Yes']['Age']) #.value_counts(sort=False))

plt.hist(y1, alpha = 0.75)
plt.hist(y2, alpha = 0.75)

plt.xlabel("Age")
plt.ylabel("Number of employees")
plt.legend(["No treatment", "Received treatment"])
plt.title("Distribution of ages")
plt.show()

"""##Plotting the relationship between remote work and treatment"""

x = df['remote_work'].unique()
y1 = np.array(df[df['treatment'] == 'No']['remote_work'].value_counts(sort=False))
y2 = np.array(df[df['treatment'] == 'Yes']['remote_work'].value_counts(sort=False))

plt.bar(x, y1, color='r')
plt.bar(x, y2, bottom=y1, color='b')

plt.xlabel("Remote work")
plt.ylabel("Number of employees")
plt.legend(["No", "Yes"])
plt.title("Treatment received by employees working remotely")
plt.show()

"""#Logistic Regression Model"""

import warnings
warnings.filterwarnings('ignore')

"""##Performing one-hot-encoding and integer encoding on X, and converting our target column to zeros and ones"""

for col in df:
  print(col,df[col].unique())

df['Country'].value_counts()

countries = ["United States","United Kingdom","Canada","Germany","Ireland","Netherlands","Australia","France","India"]
for i in range(len(df)):
  if (df.iloc[i, 2] not in countries):
    df.iloc[i, 2] = 'Other'
df['Country'].unique()

# dataframe of just categorical features
df_categorical = df[['Gender','Country']]

# one-hot encoding categorical features
dummies = pd.get_dummies(df_categorical)
dummies.head()
# dropping the original columns from the dataframe
df = df.drop(['Gender','Country'], axis = 1)

# appending dummies (one-hot encoded categories) to dataframe
df = pd.concat([df, dummies], axis=1)

df.head()

from sklearn.preprocessing import LabelEncoder
df = df.apply(LabelEncoder().fit_transform)

for col in df:
  print(col,df[col].unique())

df.head()

"""##Splitting data into X and y values
X includes all columns except treatment column, and y is only the treatment column.
"""

X = df.drop(['treatment'], axis=1)
Y = df['treatment']
X.shape

Y.shape

"""##Train/test split on X and y values"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

"""##Defining and fitting a logistic regression model"""

X_train = np.array(X_train)
y_train = np.array(y_train).reshape(-1, 1)
X_test = np.array(X_test)
y_test = np.array(y_test)
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

from sklearn.linear_model import LogisticRegression

logit_model = LogisticRegression()
logit_model.fit(X_train, y_train)

"""##Coefficients of the model"""

logit_model.coef_

"""##Finding the accuracy of the model
The accuracy of the model is the number of correct predictions over the total number of predictions.
"""

y_test_pred = logit_model.predict(X_test)

y_test_pred

np.sum(y_test_pred == y_test)/len(y_test)

"""##Creating a confusion matrix
The confusion matrix helps find the number of false negatives and false positives. There are 45 false negatives and 26 false positives.
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_test_pred)
ConfusionMatrixDisplay(confusion_matrix=cm).plot()

"""## Finding the best combination of dataset features for the logistic regression model"""

def logistic_regression_model(X, Y, test_size):
  # 1. Separate training and test set data
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

  # 2. Ensure data is in the numpy array
  X_train = np.array(X_train)
  y_train = np.array(y_train)
  X_test = np.array(X_test)
  y_test = np.array(y_test)

  # 3. Define model and fit the training set data
  logit_model = LogisticRegression()
  logit_model.fit(X_train, y_train)

  # 4. Compute predicted values using training set X
  y_train_pred = logit_model.predict(X_train)

  # 5. Compute predicted values using test set X
  y_test_pred = logit_model.predict(X_test)

  test_acc = np.sum(y_test_pred == y_test)/len(y_test)
  train_acc = np.sum(y_train_pred == y_train)/len(y_train)

  return {
      "model": logit_model,
      "accuracy": [train_acc, test_acc],
      "test_predictions": [y_test, y_test_pred]
  }

#Test
test_result = logistic_regression_model(X, Y, 0.3)
test_result['accuracy']

results = []
models = []
preds = []

for i in range(1000):
  sampleX = X.sample(n=np.random.choice(list(range(1, X.shape[1]))), axis='columns')
  test_result = logistic_regression_model(sampleX, Y, 0.2)
  results.append(test_result['accuracy'][1])
  models.append(test_result['model'])
  preds.append(test_result['test_predictions'])

np.argmax(results)

"""##Best model"""

results[np.argmax(results)]

y_test = preds[np.argmax(results)][0]
y_test_pred = preds[np.argmax(results)][1]
cm = confusion_matrix(y_test, y_test_pred)
ConfusionMatrixDisplay(confusion_matrix=cm).plot()

"""##Exporting cleaned data"""

df.to_csv('/content/drive/My Drive/survey_cleaned2.csv',index =False)

"""#Neural Network"""

df = pd.read_csv('/content/drive/My Drive/survey_cleaned2.csv')
import tensorflow as tf # for neural network models
print(tf.__version__)

"""##Splitting dataset into X and y values"""

X = df.drop(['treatment'], axis=1)
Y = df['treatment']
X.shape

"""##Normalizing each column in  ð‘‹  to be between 0 and 1"""

# Your code here
X = (X-X.min())/(X.max()-X.min())

X['Age'].max()

"""##Train/test split on the $X$ and $y$ values"""

# Your code here

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

# Your code here

X_train = np.array(X_train)
y_train = np.array(y_train)

X_test = np.array(X_test)
y_test = np.array(y_test)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

"""##Building and training a simple neural network"""

model = tf.keras.models.Sequential([
  #tf.keras.layers.Dense(100, activation='relu'), # hidden layer
  #tf.keras.layers.Dense(75, activation='relu'), # hidden layer
  tf.keras.layers.Dense(15, activation='relu'), # hidden layer
  tf.keras.layers.Dense(2, activation='softmax') # output layer

])

"""##Accuracy of the best neural network model"""

# compile model
model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics=['accuracy'])

model.fit(X_train, y_train, epochs = 15)

# Get predictions on test set
predictions = model.predict(X_test)
predictions = np.argmax(predictions, axis = 1)
# Find accuracy on test set
print('Test accuracy: ', np.mean(y_test == predictions))

"""##Creating a confusion matrix for the model"""

# Your code here
cm = confusion_matrix(y_test, predictions)
ConfusionMatrixDisplay(confusion_matrix=cm).plot()

# percent of false positives
26/(26+45)

"""##Fine-tuning model architecture"""

# Split train set into a train and validation set
from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, random_state = 0, test_size = 0.2)
X_train.shape, X_valid.shape

#define a helper function that will build the model inside a loop.
def build_model(n_hidden=1, n_neurons=30):
    model = tf.keras.models.Sequential()

    # Start by flattening image
    model.add(tf.keras.layers.Flatten())

    # Add n_hidden number of hidden layers, each with n_neurons
    for layer in range(n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation="relu"))

    # Output layer -> 10 neurons for 10 digits, with softmax activation
    model.add(tf.keras.layers.Dense(10, activation = 'softmax'))

    # Compile model
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer='adam',
                  metrics=['accuracy'])

    return model

num_layers = [1,3,5,8,15,20]
num_neurons = [10,30,50,70,100,110]


for n_hidden in num_layers:

  for n_neurons in num_neurons:
    model = build_model(n_hidden, n_neurons)

    print('Number of hidden layers: ', n_hidden)
    print('Number of neurons per layer: ', n_neurons)
    history = model.fit(X_train, y_train, epochs=10,
                    validation_data=[X_valid, y_valid])

"""##Neural network visualization
This will help us visualize the loss and accuracy of the best neural network model.
"""

import tensorflow as tf # for neural network models
print(tf.__version__)

X = df.drop(['treatment'], axis=1)
Y = df['treatment']
X.shape

X = (X - X.min())/(X.max()-X.min())

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

X_train = np.array(X_train)
y_train = np.array(y_train)

X_test = np.array(X_test)
y_test = np.array(y_test)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(5, activation='relu'), # hidden layer
  tf.keras.layers.Dense(5, activation='relu'), # hidden layer
  tf.keras.layers.Dense(5, activation='relu'), # hidden layer
  tf.keras.layers.Dense(10, activation='relu'), # hidden layer
  tf.keras.layers.Dense(10, activation='relu'), # hidden layer
  tf.keras.layers.Dense(15, activation='relu'), # hidden layer
  tf.keras.layers.Dense(2, activation='softmax') # output layer
])

model.compile(
    optimizer = 'adam', # for this course, we always use adam
    loss = 'sparse_categorical_crossentropy', # since this is classification, we use categorical crossentropy (rather than MSE)
    metrics=['accuracy']) # we'll evaluate based on the accuracy of our predictions

history = model.fit(X_train, y_train, validation_split = 0.1, epochs = 25) # have to assign the fit function to a variable

import keras
from matplotlib import pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

import keras
from matplotlib import pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# Get predictions on test set
predictions = model.predict(X_test) # gives us predicted probabilities for each digit
predictions = np.argmax(predictions, axis = 1) # makes predicted digit the classification with highest probability

# Find accuracy on test set
print('Test accuracy: ', np.mean(y_test == predictions))

"""#Random Forest"""

df = pd.read_csv('/content/drive/My Drive/survey_cleaned2.csv')
from sklearn import tree

"""##Split dataset into X and y values"""

X = df.drop(['treatment'], axis=1)
Y = df['treatment']
X.shape

"""##Train/test split on the $X$ and $y$ values"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

X_train = np.array(X_train)
y_train = np.array(y_train)

X_test = np.array(X_test)
y_test = np.array(y_test)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

"""##Creating and training a decision tree"""

from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train, y_train)

"""##Creating and training a random forest and assessing performance
A Random Forest is a series of decision trees.
"""

y_test_pred = clf.predict(X_test)
np.sum(y_test_pred == y_test)/len(y_test)

from sklearn.ensemble import RandomForestClassifier
for x in range(1,30):
  clf = RandomForestClassifier(max_depth=x, random_state=0)
  clf.fit(X_train, y_train)
  y_test_pred = clf.predict(X_test)
  print('Number of trees: ', x)
  print(np.sum(y_test_pred == y_test)/len(y_test))